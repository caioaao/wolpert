{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to implement the algorithm described in [Breiman's Stacked regression paper](https://link.springer.com/content/pdf/10.1007/BF00117832.pdf). The idea behind it is basicaly to rely on [ridge regression](https://en.wikipedia.org/wiki/Tikhonov_regularization) to select coefficients for the predictions on the first layer of the stacked model. Ridge is selected because it can generate coefficients that are equal to zero, thus selecting predictions.\n",
    "\n",
    "Another interesting idea in the paper is to make it so the ridge regression only produces non-negative coefficients. The explanation from the paper is:\n",
    "\n",
    "> Only partial answers are available. Suppose that the $v_k(x)$ are strongly correlated and the {$\\alpha_k$} are chosen using least squares or ridge regression. Then there is no guarantee that the resulting predictor ~k akvk (x) will stay near the range [mink vk (x), maxk vk (x)] and generalization may be poor.\n",
    "\n",
    "Where $v_k$ is the k-th model on the first layer, $x$ is the original data and $\\alpha_k$ is the k-th coefficient from the regression.\n",
    "\n",
    "For the experiments, we'll use the same data set as the paper (Boston Housing), available on scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_STATE = 3824500\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=50, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm described in the paper for growing trees is too complicated for this example, so we'll simplify it. Instead, we'll just use a bunch of [random forest regressors](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) with different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296 Random forest regressors on the first layer\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "rf_param_grid = {\"n_estimators\": [10, 50, 100, 300],\n",
    "                 \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                 \"max_depth\": [1, 3, 9, None],\n",
    "                 \"min_samples_split\": [2, 5, 10],\n",
    "                 \"min_samples_leaf\": [1, 2, 4],\n",
    "                 \"n_jobs\": [1],\n",
    "                 \"random_state\": np.random.randint(1000000, size=3)}\n",
    "\n",
    "tree_configs = ParameterGrid(rf_param_grid)\n",
    "\n",
    "regs = [RandomForestRegressor(**params) for params in tree_configs]\n",
    "print(\"%d Random forest regressors on the first layer\" % len(regs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build the first layer using wolpert. To make it as close as possible to the paper, we'll use a cross validation approach with 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wolpert.wrappers import CVWrapper\n",
    "from wolpert.pipeline import make_stack_layer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "wrapper = CVWrapper(cv=cv, default_method='predict', n_cv_jobs=-1)\n",
    "\n",
    "layer0 = make_stack_layer(*regs, blending_wrapper=wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things easier, we'll use [scikit learn's ElasticNet](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) as it already implements the non-negative constraint. To make it behave as a ridge regression, all we have to do is remove the L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from wolpert.pipeline import StackingPipeline\n",
    "\n",
    "meta_reg = ElasticNet(l1_ratio=0., random_state=RANDOM_STATE, positive=True)\n",
    "\n",
    "final_reg = StackingPipeline([(\"l0\", layer0), (\"meta\", meta_reg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caio/anaconda3/envs/wolpert_devel/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingPipeline(memory=None,\n",
       "         steps=[('l0', StackingLayer(n_jobs=1,\n",
       "       transformer_list=[('randomforestregressor-1', CVStackableTransformer(cv=KFold(n_splits=10, random_state=3824500, shuffle=True),\n",
       "            estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=1,\n",
       "           max_features='auto', max_l...compute=False,\n",
       "      random_state=3824500, selection='cyclic', tol=0.0001,\n",
       "      warm_start=False))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reg.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the MSE on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE for stacked ensemble: 5.36880\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds_test = final_reg.predict(Xtest)\n",
    "\n",
    "print(\"Test MSE for stacked ensemble: %.5f\" % mean_squared_error(ytest, preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the best MSE by checking the blended results from layer 0 against the real values, as each column is the cross validation predictions of a single estimator.\n",
    "\n",
    "* __Note:__ we have to use the cross validation scores to choose the best estimator on the first layer, not just selecting the one that gives the best one in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MSE for best regressor on first layer is 11.05074\n",
      "Test MSE for best regressor on first layer is 5.61604\n"
     ]
    }
   ],
   "source": [
    "Xt_train, train_indexes = layer0.fit_blend(Xtrain, ytrain)\n",
    "\n",
    "best = -1\n",
    "best_mse = 1e20\n",
    "\n",
    "for i in range(Xt_train.shape[1]):\n",
    "    candidate_mse = mean_squared_error(ytrain[train_indexes], Xt_train[:, i])\n",
    "    if candidate_mse < best_mse:\n",
    "        best_mse = candidate_mse\n",
    "        best = i\n",
    "        \n",
    "print(\"CV MSE for best regressor on first layer is %.5f\" % best_mse)\n",
    "\n",
    "Xt_test = layer0.transform(Xtest)\n",
    "mse_test = mean_squared_error(ytest, Xt_test[:, best])\n",
    "print(\"Test MSE for best regressor on first layer is %.5f\" % mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the article, the best regressor on the first layer is still worse than the stacked regression.\n",
    "\n",
    "We can also check that the ridge regressor is selecting a small subset of models by looking at the sum of its coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of ridge regressor's coefficients: 1.11493\n"
     ]
    }
   ],
   "source": [
    "sum_coefs = np.sum(final_reg.named_steps[\"meta\"].coef_)\n",
    "\n",
    "print(\"Sum of ridge regressor's coefficients: %.5f\" % sum_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means that only around 1% of the base models were used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1274"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(final_reg.named_steps[\"meta\"].coef_ == 0.).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
